{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9027459",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.ans--Cyclic learning rates (and cyclic momentum, which usually goes hand-in-hand) is a learning rate scheduling technique for (1) faster training of a network and (2) a finer understanding of the optimal learning rate\n",
    "\n",
    "2.Ans--What callback keeps track of hyperparameter values (along with other data) during\n",
    "training\n",
    "\n",
    "3.Ans--In the color dim plot, what does one column of pixels represent\n",
    "\n",
    "4.Ans-In color dim, what does &quot;poor teaching&quot; look like? What is the reason for this\n",
    "\n",
    "5.Ans--Just like the parameters (eg. weights, bias) of any network layer, a Batch Norm layer also has parameters of its own: Two learnable parameters called beta and gamma.\n",
    "    \n",
    "    \n",
    "6.Ans--Batch normalization is a technique to standardize the inputs to a network, applied to ether the activations of a prior layer or inputs directly. Batch normalization accelerates training, in some cases by halving the epochs or better, and provides some regularization, reducing generalization error.\n",
    "\n",
    "\n",
    "7.Ans--Batch normalization solves a major problem called internal covariate shift. It helps by making the data flowing between intermediate layers of the neural network look, this means you can use a higher learning rate. It has a regularizing effect which means you can often remove dropout.\n",
    "\n",
    "\n",
    "8.Ans--Explain between MAX POOLING and AVERAGE POOLING is number eight.\n",
    "\n",
    "\n",
    "9.Ans--Pooling Layer The main purpose of pooling layer is to progressively reduce the spatial size of the input image, so that number of computations in the network are reduced. Pooling performs downsampling by reducing the size and sends only the important data to next layers in CNN.\n",
    "\n",
    "\n",
    "10.Ans--Fully connected layers are global (they can introduce any kind of dependence). This is also why convolutions work so well in domains like image analysis - due to their local nature they are much easier to train, even though mathematically they are just a subset of what fully connected layers can represent.\n",
    "\n",
    "\n",
    "11.Ans--A parameter is a limit. In mathematics a parameter is a constant in an equation, but parameter isn't just for math anymore: now any system can have parameters that define its operation. You can set parameters for your class debate\n",
    "\n",
    "\n",
    "12.Ans--What formulas are used to measure these PARAMETERS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

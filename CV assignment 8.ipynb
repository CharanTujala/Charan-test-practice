{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.ans-Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE.\n",
    "\n",
    "2.Ans--An Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer.\n",
    "\n",
    "3.Ans--What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL)\n",
    "\n",
    "4.Ans-THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE\n",
    "\n",
    "5.ANS--Mention three components. Style GoogLeNet\n",
    "\n",
    "6.ANS-Using our own terms and diagrams, explain RESNET ARCHITECTURE.\n",
    "\n",
    "7.Ans--What are Skip Connections? Skip Connections (or Shortcut Connections) as the name suggests skips some of the layers in the neural network and feeds the output of one layer as the input to the next layers. Skip Connections were introduced to solve different problems in different architectures.\n",
    "\n",
    "8.Ans--A residual block is a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. The non-linearity is then applied after adding it together with the output of the corresponding layer in the main path\n",
    "\n",
    "9.Ans-Transfer learning is a technique to help solve this problem. As a concept, it works by transferring as much knowledge as possible from an existing model to a new model designed for a similar task. For example, transferring the more general aspects of a model which make up the main processes for completing a task.\n",
    "\n",
    "10.Ans--In other words, transfer learning is a machine learning method where we reuse a pre-trained model as the starting point for a model on a new task. To put it simplyâ€”a model trained on one task is repurposed on a second, related task as an optimization that allows rapid progress when modeling the second task\n",
    "\n",
    "11.Ans-WHY IS FINE-TUNING BETTER THAN START-UP TRAINING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

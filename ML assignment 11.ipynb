{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Ans--Prior probability shows the likelihood of an outcome in a given dataset. For example, in the mortgage case, P(Y) is the default rate on a home mortgage, which is 2%. P(Y|X) is called the conditional probability, which provides the probability of an outcome given the evidence, that is, when the value of X is known\n",
    "\n",
    "\n",
    "2.Ans--Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the probability that urn A is chosen is 0.5.\n",
    "\n",
    "\n",
    "3.Ans--Suppose we have a coin that is assumed to be fair. If we flip the coin one time, the probability that it will land on heads is 0.5. Now suppose we flip the coin 100 times and it only lands on heads 17 times. We would say that the likelihood that the coin is fair is quite low.\n",
    "\n",
    "\n",
    "4.Ans--Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data; however, the technique is very effective on a large range of complex problems.\n",
    "\n",
    "\n",
    "5.Ans-Bayes Optimal Classifier is a probabilistic model that finds the most probable prediction using the training data and space of hypotheses to make a prediction for a new data instance\n",
    "\n",
    "\n",
    "6.Ans--Features of Bayesian learning methods:\n",
    "– This provides a more flexible approach to learning than algorithms that completely eliminate a hypothesis if it is found to be inconsistent with any single example. – a probability distribution over observed data for each possible hypothesis.\n",
    "\n",
    "\n",
    "7.Ans--Consistent Learners. • A learner L using a hypothesis H and training data D is said to be a consistent learner if it always outputs a hypothesis with zero error on D whenever H contains such a hypothesis. • By definition, a consistent learner must produce a hypothesis in the version space for H given D.\n",
    "\n",
    "\n",
    "8.Ans-Advantages of Naive Bayes Classifier\n",
    "It is simple and easy to implement.\n",
    "It doesn't require as much training data.\n",
    "It handles both continuous and discrete data.\n",
    "\n",
    "\n",
    "9.Ans--Disadvantages of Using Naive Bayes Classifier\n",
    "Bad binning of continuous variables with Multinomial naive bayes: Gaussian Naive Bayes.\n",
    "Not great for imbalanced data: Complement Naive Bayes.\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a510b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Ans--What are the key reasons for reducing the dimensionality of a dataset? What are the major\n",
    "disadvantages\n",
    "\n",
    "2.Ans--What is the dimensionality curse\n",
    "\n",
    "3.Ans--Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how\n",
    "can you go about doing it? If not, what is the reason\n",
    "\n",
    "4.Ans--PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear because it can at least get rid of useless dimensions. However, if there are no useless dimensions, reducing dimensionality with PCA will lose too much information.\n",
    "\n",
    "5.Ans--Assume you&#39;re running PCA on a 1,000-dimensional dataset with a 95 percent explained variance\n",
    "ratio. What is the number of dimensions that the resulting dataset would have\n",
    "\n",
    "6.Ans-Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations\n",
    "\n",
    "7.Ans-How do you assess a dimensionality reduction algorithm&#39;s success on your dataset\n",
    "\n",
    "8.Ans--Is it logical to use two different dimensionality reduction algorithms in a chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

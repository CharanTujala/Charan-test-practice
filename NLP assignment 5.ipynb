{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Ans--A Seq2Seq model is a model that takes a sequence of items (words, letters, time series, etc) and outputs another sequence of items. Seq2Seq Model. In the case of Neural Machine Translation, the input is a series of words, and the output is the translated series of words.\n",
    "\n",
    "2.Ans-However, RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done.\n",
    "\n",
    "3.Ans--Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and using the clipped gradients to update the weights.\n",
    "\n",
    "\n",
    "4.Ans-The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all of the encoded input vectors, with the most relevant vectors being attributed the highest weights\n",
    "\n",
    "\n",
    "5.Ans--Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account.\n",
    "\n",
    "6.Ans--Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence\n",
    "\n",
    "7.Ans-Bahdanau attention mechanism\n",
    "proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.\n",
    "\n",
    "\n",
    "8.Ans-What is a Language Model\n",
    "\n",
    "9.Ans-Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension.\n",
    "\n",
    "\n",
    "10.Ans--What is Bilingual Evaluation Understudy (BLEU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

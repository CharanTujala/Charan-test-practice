{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Ans--BERT is basically an Encoder stack of transformer architecture. A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack.\n",
    "\n",
    "2.Ans--Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary.\n",
    "\n",
    "3.Ans--Next sentence prediction (NSP) is one-half of the training process behind the BERT model (the other being masked-language modeling — MLM). Where MLM teaches BERT to understand relationships between words — NSP teaches BERT to understand longer-term dependencies across sentences\n",
    "\n",
    "4.Ans--Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table\n",
    "\n",
    "5.Ans--What is Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "6.Ans--Explain Semantic Role Labeling\n",
    "\n",
    "\n",
    "7.Ans-Why Fine-tuning a BERT model takes less time than pretraining\n",
    "\n",
    "\n",
    "8.Ans-Recognizing Textual Entailment (RTE)\n",
    "\n",
    "9.Ans--GPT model was based on Transformer architecture. It was made of decoders stacked on top of each other (12 decoders). These models were same as BERT as they were also based on Transformer architecture. The difference in architecture with BERT is that it used stacked encoder layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

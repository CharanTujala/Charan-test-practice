{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.ans--A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
    "\n",
    "2.ans-A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words\n",
    "    \n",
    "3.Ans--Description. A bag-of-n-grams model records the number of times that each n-gram appears in each document of a collection. An n-gram is a collection of n successi\n",
    "\n",
    "\n",
    "\n",
    "4.Ans--Explain TF-IDF\n",
    "\n",
    "5.Ans--Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. In speech recognition, it's the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning\n",
    "    \n",
    "6.ans-Thus by using word embeddings, words that are close in meaning are grouped near to one another in vector space. For example, while representing a word such as frog, the nearest neighbour of a frog would be frogs, toads, Litoria\n",
    "\n",
    "7.ans--Continuous Bag of Words Model (CBOW) and Skip-gram\n",
    "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle . While in the Skip-gram model, the distributed representation of the input word is used to predict the context \n",
    "\n",
    "\n",
    "8.ans--Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It's reverse of CBOW algorithm. Here, target word is input while context words are output.\n",
    "\n",
    "\n",
    "9.ans--GloVe stands for Global Vectors for word representation. It is an unsupervised learning algorithm developed by researchers at Stanford University aiming to generate word embeddings by aggregating global word co-occurrence matrices from a given corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
